{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fer = pd.read_csv(\"C:/Users/Teja/Desktop/teja/Education/Projects/FER/fer2013/fer2013.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 48, 48\n",
    "faces_pixels = fer['pixels'].tolist()\n",
    "faces = []\n",
    "\n",
    "for face in faces_pixels:\n",
    "    face = face.split(\" \")\n",
    "    face = np.asarray(face).reshape(width, height)\n",
    "    faces.append(face.astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35887, 48, 48, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces = np.array(faces)\n",
    "faces = np.expand_dims(faces, -1)\n",
    "faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teja\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "emotion = pd.get_dummies(fer['emotion']).as_matrix()\n",
    "# emotion.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(faces, emotion, test_size = 0.1, random_state = 123)\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = [None, 48*48], name = \"X\")\n",
    "x = tf.reshape(x, [-1, 48, 48, 1])\n",
    "\n",
    "y = tf.placeholder(tf.float32, shape = [None, 7], name = \"Y\")\n",
    "y_image = tf.argmax(y, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv(input, w, b, stride=1):\n",
    "    layer = tf.nn.conv2d(input, w, strides = [1, stride, stride, 1], padding= \"SAME\")\n",
    "    layer = tf.nn.bias_add(layer, b)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_layer(input, stride = 2):\n",
    "    layer = tf.nn.max_pool(input, ksize=[1, stride, stride, 1], strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Teja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Initializing parameters\n",
    "weights = {\n",
    "    'W0' : tf.Variable(tf.truncated_normal([5, 5, 1, 64], stddev = 0.05), name = 'w0'),\n",
    "    'W1' : tf.Variable(tf.truncated_normal([5, 5, 64, 128], stddev = 0.05), name = 'w1'),\n",
    "    'W2' : tf.Variable(tf.truncated_normal([5, 5, 128, 256], stddev = 0.05), name = 'w2'),\n",
    "    'W3' : tf.Variable(tf.truncated_normal([6*6*256, 4096], stddev = 0.05), name = 'w3'),\n",
    "    'W4' : tf.Variable(tf.truncated_normal([4096, 1024], stddev = 0.05), name = 'w4'),\n",
    "    'W5' : tf.Variable(tf.truncated_normal([1024, 256], stddev = 0.05), name = 'w5'),\n",
    "    'W6' : tf.Variable(tf.truncated_normal([256, 7], stddev = 0.05), name = 'w6')\n",
    "}\n",
    "biases = {\n",
    "    'B0' : tf.Variable(tf.constant(0.05, shape= [64]), name = 'b0'),\n",
    "    'B1' : tf.Variable(tf.constant(0.05, shape= [128]), name = 'b1'),\n",
    "    'B2' : tf.Variable(tf.constant(0.05, shape= [256]), name = 'b2'),\n",
    "    'B3' : tf.Variable(tf.constant(0.05, shape= [4096]), name = 'b3'),\n",
    "    'B4' : tf.Variable(tf.constant(0.05, shape= [1024]), name = 'b4'),\n",
    "    'B5' : tf.Variable(tf.constant(0.05, shape= [256]), name = 'b5'),\n",
    "    'B6' : tf.Variable(tf.constant(0.05, shape= [7]), name = 'b6')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = {\n",
    "    'W0' : tf.Variable(tf.truncated_normal([3, 3, 1, 64], stddev = 0.05), name = 'w0'),\n",
    "    'W1' : tf.Variable(tf.truncated_normal([3, 3, 64, 64], stddev = 0.05), name = 'w0'),\n",
    "    'W2' : tf.Variable(tf.truncated_normal([3, 3, 64, 128], stddev = 0.05), name = 'w0'),\n",
    "    'W3' : tf.Variable(tf.truncated_normal([3, 3, 128, 128], stddev = 0.05), name = 'w0'),\n",
    "    'W4' : tf.Variable(tf.truncated_normal([3, 3, 128, 256], stddev = 0.05), name = 'w0'),\n",
    "    'W5' : tf.Variable(tf.truncated_normal([3, 3, 256, 256], stddev = 0.05), name = 'w0'),\n",
    "    'W6' : tf.Variable(tf.truncated_normal([3, 3, 256, 512], stddev = 0.05), name = 'w0'),\n",
    "    'W7' : tf.Variable(tf.truncated_normal([3, 3, 512, 512], stddev = 0.05), name = 'w0'),\n",
    "    'W8' : tf.Variable(tf.truncated_normal([3*3*512, 512], stddev = 0.05), name = 'w0'),\n",
    "    'W9' : tf.Variable(tf.truncated_normal([512, 256], stddev = 0.05), name = 'w0'),\n",
    "    'W10' : tf.Variable(tf.truncated_normal([256, 128], stddev = 0.05), name = 'w0'),\n",
    "    'W11' : tf.Variable(tf.truncated_normal([128, 7], stddev = 0.05), name = 'w0')\n",
    "}\n",
    "\n",
    "biases1 = {\n",
    "    'B0' : tf.Variable(tf.constant(0.05, shape= [64]), name = 'b0'),\n",
    "    'B1' : tf.Variable(tf.constant(0.05, shape= [64]), name = 'b1'),\n",
    "    'B2' : tf.Variable(tf.constant(0.05, shape= [128]), name = 'b2'),\n",
    "    'B3' : tf.Variable(tf.constant(0.05, shape= [128]), name = 'b3'),\n",
    "    'B4' : tf.Variable(tf.constant(0.05, shape= [256]), name = 'b4'),\n",
    "    'B5' : tf.Variable(tf.constant(0.05, shape= [256]), name = 'b5'),\n",
    "    'B6' : tf.Variable(tf.constant(0.05, shape= [512]), name = 'b6'),\n",
    "    'B7' : tf.Variable(tf.constant(0.05, shape= [512]), name = 'b7'),\n",
    "    'B8' : tf.Variable(tf.constant(0.05, shape= [512]), name = 'b8'),\n",
    "    'B9' : tf.Variable(tf.constant(0.05, shape= [256]), name = 'b9'),\n",
    "    'B10' : tf.Variable(tf.constant(0.05, shape= [128]), name = 'b10'),\n",
    "    'B11' : tf.Variable(tf.constant(0.05, shape= [7]), name = 'b11')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Convolution Nueral Network\n",
    "def CNN(x, weights, biases):\n",
    "    # First Convolution Layer\n",
    "    layer1 = new_conv(x, weights['W0'], biases['B0'])\n",
    "    layer1 = pool_layer(layer1)\n",
    "    \n",
    "    #Second Convolution Layer\n",
    "    layer2 = new_conv(layer1, weights['W1'], biases['B1'])\n",
    "    layer2 = pool_layer(layer2)\n",
    "    \n",
    "    # Third Convolution Layer\n",
    "    layer3  = new_conv(layer2, weights['W2'], biases['B2'])\n",
    "    layer3 = pool_layer(layer3)\n",
    "    \n",
    "    # Flattening the Convolution Layer\n",
    "    layer4 = tf.reshape(layer3, shape = [-1, weights['W3'].get_shape().as_list()[0]])\n",
    "    \n",
    "    # Dropout \n",
    "    layer4 = tf.nn.dropout(layer4, rate= .3, seed = 1)\n",
    "    \n",
    "    # Fully Connected layer 1\n",
    "    fc1 = tf.add(tf.matmul(layer4, weights['W3']), biases['B3'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    #Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, rate= .3, seed = 2) \n",
    "    \n",
    "    # Fully Connected Layer 2\n",
    "    fc2 = tf.add(tf.matmul(fc1, weights['W4']), biases['B4'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Dropout\n",
    "    fc2 = tf.nn.dropout(fc2, rate= .3, seed = 3)\n",
    "    \n",
    "    # Fully Connected layer 3\n",
    "    fc3 = tf.add(tf.matmul(fc2, weights['W5']), biases['B5'])\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    \n",
    "    # Dropout\n",
    "    fc3 = tf.nn.dropout(fc3, rate= .3, seed = 4) \n",
    "    \n",
    "    # Fully Connected Layer 4\n",
    "    out  = tf.add(tf.matmul(fc3, weights['W6']), biases['B6'])\n",
    "#     out = tf.nn.softmax(out)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = CNN(x, weights, biases)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-f518f3e267a5>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "matches = tf.equal(tf.argmax(tf.nn.softmax(pred), 1), y_image)\n",
    "accuracy = tf.reduce_mean(tf.cast(matches, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_loss = []\n",
    "    testing_loss = []\n",
    "    train_accuracy = []\n",
    "    testing_accuracy = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        start_time = time.time()\n",
    "        loss, acc = 0, 0\n",
    "        for batch in range(len(X_train)// batch_size):\n",
    "            X_batch = X_train[batch * batch_size : min((batch+1) * batch_size, len(X_train))]\n",
    "            Y_batch = Y_train[batch * batch_size : min((batch+1) * batch_size, len(Y_train))]\n",
    "            \n",
    "            # Running Optimizer\n",
    "            opt = sess.run(optimizer, feed_dict = {x: X_batch, y: Y_batch})\n",
    "            \n",
    "            # Running cost function and cost\n",
    "            loss += sess.run(cost, feed_dict = {x: X_batch, y: Y_batch})\n",
    "            acc += sess.run(accuracy, feed_dict = {x: X_batch, y: Y_batch})\n",
    "        loss /= int(len(X_train)/batch_size)\n",
    "        acc /= int(len(X_train)/batch_size)\n",
    "        test_loss, test_acc = sess.run([cost, accuracy], feed_dict = {x: X_test, y: Y_test})\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(\"Epochs \" + str(i+1) + \" completed: \" + \"Time Usage {:.3f}\".format(end_time - start_time))\n",
    "        print(\"Accuracy:\")\n",
    "        print(\"\\t* Training Accuracy:\\t{:.6f}\".format(acc))\n",
    "        print(\"\\t* Test Accuracy:\\t{:.6f}\".format(test_acc))\n",
    "        print(\"Loss:\")\n",
    "        print(\"\\t* Training Loss:\\t{:.6f}\".format(loss))\n",
    "        print(\"\\t* Test Loss:\\t{:.6f}\".format(test_loss))\n",
    "        \n",
    "        train_loss.append(loss)\n",
    "        testing_loss.append(test_loss)\n",
    "        train_accuracy.append(acc)\n",
    "        testing_accuracy.append(test_acc)\n",
    "        saver.save(sess, 'my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss)), train_loss, 'b', label = \"Training Loss\")\n",
    "plt.plot(range(len(testing_loss)), testing_loss, 'r', label = \"Test Loss\")\n",
    "plt.title(\"Training Loss Vs Test Loss\")\n",
    "plt.xlabel(\"no of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_accuracy)), train_accuracy, 'b', label = \"Training Accuracy\")\n",
    "plt.plot(range(len(testing_accuracy)), testing_accuracy, 'r', label = \"Test Loss\")\n",
    "plt.title(\"Training Accuracy Vs Validation Accuracy\")\n",
    "plt.xlabel(\"no of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN1(x, weights1, biases1):\n",
    "    # First Convolution Layer\n",
    "    layer1 = new_conv(x, weights1['W0'], biases1['B0'])\n",
    "    layer1 = new_conv(layer1, weights1['W1'], biases1['B1'])\n",
    "    layer1 = pool_layer(layer1)\n",
    "    layer1 = tf.nn.dropout(layer1, rate= .3, seed = 143)\n",
    "    \n",
    "    # Second Convolution Layer\n",
    "    layer2 = new_conv(layer1, weights1['W2'], biases1['B2'])\n",
    "    layer2 = new_conv(layer2, weights1['W3'], biases1['B3'])\n",
    "    layer2 = pool_layer(layer2)\n",
    "    layer2 = tf.nn.dropout(layer2, rate= .3, seed = 143)\n",
    "    \n",
    "    # Third Convolution Layer\n",
    "    layer3 = new_conv(layer2, weights1['W4'], biases1['B4'])\n",
    "    layer3 = new_conv(layer3, weights1['W5'], biases1['B5'])\n",
    "    layer3 = pool_layer(layer3)\n",
    "    layer3 = tf.nn.dropout(layer3, rate= .3, seed = 143)\n",
    "    \n",
    "    # Fourth Convolution Layer\n",
    "    layer4 = new_conv(layer3, weights1['W6'], biases1['B6'])\n",
    "    layer4 = new_conv(layer4, weights1['W7'], biases1['B7'])\n",
    "    layer4 = pool_layer(layer4)\n",
    "    layer4 = tf.nn.dropout(layer4, rate= .3, seed = 143)\n",
    "    \n",
    "    # Flattening the layer4\n",
    "    fc1 = tf.reshape(layer4, shape = [-1, weights1['W8'].get_shape().as_list()[0]])\n",
    "    \n",
    "    # Fully Connected layer 1\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights1['W8']), biases1['B8'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, rate= .3, seed = 143)\n",
    "    \n",
    "    # Fully Connected layer 2\n",
    "    fc2 = tf.add(tf.matmul(fc1, weights1['W9']), biases1['B9'])\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, rate= .3, seed = 143)\n",
    "    \n",
    "    # Fully Connected layer 3\n",
    "    fc3 = tf.add(tf.matmul(fc2, weights1['W10']), biases1['B10'])\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    fc3 = tf.nn.dropout(fc3, rate= .3, seed = 143)\n",
    "    \n",
    "    # Fully Connected layer 4\n",
    "    fc4 = tf.add(tf.matmul(fc3, weights1['W11']), biases1['B11'])\n",
    "    \n",
    "    return fc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = CNN1(x, weights1, biases1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_logits(logits= pred1, labels = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = tf.train.AdamOptimizer(learning_rate =  learning_rate).minimize(cost1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches1 = tf.equal(tf.argmax(pred1, 1), y_image)\n",
    "accuracy1 = tf.reduce_mean(tf.cast(matches1, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_loss1 = []\n",
    "    test_loss1 = []\n",
    "    train_accuracy1 = []\n",
    "    test_accuracy1 = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        start_time = time.time()\n",
    "        loss, acc = 0, 0\n",
    "        for batch in range(len(X_train)// batch_size):\n",
    "            X_batch = X_train[batch * batch_size : min((batch+1) * batch_size, len(X_train))]\n",
    "            Y_batch = Y_train[batch * batch_size : min((batch+1) * batch_size, len(Y_train))]\n",
    "            \n",
    "            # Running Optimizer\n",
    "            opt = sess.run(optimizer1, feed_dict = {x: X_batch, y: Y_batch})\n",
    "            \n",
    "            # Running cost function and cost\n",
    "            loss += sess.run(cost1, feed_dict = {x: X_batch, y: Y_batch})\n",
    "            acc += sess.run(accuracy1, feed_dict = {x: X_batch, y: Y_batch})\n",
    "        loss /= int(len(X_train)/batch_size)\n",
    "        acc /= int(len(X_train)/batch_size)\n",
    "        test_loss, test_acc = sess.run([cost1, accuracy1], feed_dict = {x: X_test, y: Y_test})\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(\"Epochs \" + str(i+1) + \" completed: \" + \"Time Usage {:.3f}\".format(end_time - start_time))\n",
    "        print(\"Accuracy:\")\n",
    "        print(\"\\t* Training Accuracy:\\t{:.6f}\".format(acc))\n",
    "        print(\"\\t* Test Accuracy:\\t{:.6f}\".format(test_acc))\n",
    "        print(\"Loss:\")\n",
    "        print(\"\\t* Training Loss:\\t{:.6f}\".format(loss))\n",
    "        print(\"\\t* Test Loss:\\t{:.6f}\".format(test_loss))\n",
    "        \n",
    "        train_loss1.append(loss)\n",
    "        test_loss1.append(test_loss)\n",
    "        train_accuracy1.append(acc)\n",
    "        test_accuracy1.append(test_acc)\n",
    "        saver.save(sess, \"my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss1)), train_loss1, 'b', label = \"Training Loss\")\n",
    "plt.plot(range(len(test_loss1)), test_loss1, 'r', label = \"Test Loss\")\n",
    "plt.title(\"Training Loss Vs Test Loss\")\n",
    "plt.xlabel(\"no of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_accuracy1)), train_accuracy1, 'b', label = \"Training Accuracy\")\n",
    "plt.plot(range(len(test_accuracy1)), test_accuracy1, 'r', label = \"Test Loss\")\n",
    "plt.title(\"Training Accuracy Vs Validation Accuracy\")\n",
    "plt.xlabel(\"no of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "res = tf.train.import_meta_graph(\"my_model.meta\")\n",
    "res.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "graph = tf.get_defaut_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    W0 = graph.get_tensor_by_name(\"w0:0\"),\n",
    "    W1 = graph.get_tensor_by_name(\"w1:0\"),\n",
    "    W2 = graph.get_tensor_by_name(\"w2:0\"),\n",
    "    W3 = graph.get_tensor_by_name(\"w3:0\"),\n",
    "    W4 = graph.get_tensor_by_name(\"w4:0\"),\n",
    "    W5 = graph.get_tensor_by_name(\"w5:0\"),\n",
    "    W6 = graph.get_tensor_by_name(\"w6:0\")\n",
    "}\n",
    "biases = {\n",
    "    B0 = graph.get_tensor_by_name(\"b0:0\"),\n",
    "    B1 = graph.get_tensor_by_name(\"b1:0\"),\n",
    "    B2 = graph.get_tensor_by_name(\"b2:0\"),\n",
    "    B3 = graph.get_tensor_by_name(\"b3:0\"),\n",
    "    B4 = graph.get_tensor_by_name(\"b4:0\"),\n",
    "    B5 = graph.get_tensor_by_name(\"b5:0\"),\n",
    "    B6 = graph.get_tensor_by_name(\"b6:0\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = {\n",
    "    W0 = graph.get_tensor_by_name(\"w0:0\"),\n",
    "    W1 = graph.get_tensor_by_name(\"w1:0\"),\n",
    "    W2 = graph.get_tensor_by_name(\"w2:0\"),\n",
    "    W3 = graph.get_tensor_by_name(\"w3:0\"),\n",
    "    W4 = graph.get_tensor_by_name(\"w4:0\"),\n",
    "    W5 = graph.get_tensor_by_name(\"w5:0\"),\n",
    "    W6 = graph.get_tensor_by_name(\"w6:0\"),\n",
    "    W7 = graph.get_tensor_by_name(\"w7:0\"),\n",
    "    W8 = graph.get_tensor_by_name(\"w8:0\"),\n",
    "    W9 = graph.get_tensor_by_name(\"w9:0\"),\n",
    "    W10 = graph.get_tensor_by_name(\"w10:0\"),\n",
    "    W11 = graph.get_tensor_by_name(\"w11:0\")\n",
    "}\n",
    "biases1 = {\n",
    "    B0 = graph.get_tensor_by_name(\"b0:0\"),\n",
    "    B1 = graph.get_tensor_by_name(\"b1:0\"),\n",
    "    B2 = graph.get_tensor_by_name(\"b2:0\"),\n",
    "    B3 = graph.get_tensor_by_name(\"b3:0\"),\n",
    "    B4 = graph.get_tensor_by_name(\"b4:0\"),\n",
    "    B5 = graph.get_tensor_by_name(\"b5:0\"),\n",
    "    B6 = graph.get_tensor_by_name(\"b6:0\"),\n",
    "    B7 = graph.get_tensor_by_name(\"b7:0\"),\n",
    "    B8 = graph.get_tensor_by_name(\"b8:0\"),\n",
    "    B9 = graph.get_tensor_by_name(\"b9:0\"),\n",
    "    B10 = graph.get_tensor_by_name(\"b10:0\"),\n",
    "    B11 = graph.get_tensor_by_name(\"b11:0\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgust\", 2: \"Fear\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprise\", 6: \"Neutral\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
